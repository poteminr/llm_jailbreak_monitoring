{
    "datasets": [
        {
            "dataset_path": "unalignment/toxic-dpo-v0.2",
            "prompt_field": "prompt",
            "output_field": "chosen",
            "label_field": null,
            "jailbreak_field": null
        },
        {
            "dataset_path": "unalignment/toxic-dpo-v0.1",
            "prompt_field": "prompt",
            "output_field": "chosen",
            "label_field": null,
            "jailbreak_field": null
        },
        {
            "dataset_path": "lmsys/toxic-chat",
            "prompt_field": "prompt",
            "output_field": null,
            "label_field": null,
            "jailbreak_field": "jailbreaking"
        },
        {
            "dataset_path": "rubend18/ChatGPT-Jailbreak-Prompts",
            "prompt_field": "prompt",
            "output_field": null,
            "label_field": null,
            "jailbreak_field": "Jailbreak Score"
        },
        {
            "dataset_path": "hackaprompt/hackaprompt-dataset",
            "prompt_field": "prompt",
            "output_field": null,
            "label_field": null,
            "jailbreak_field": "correct"
        }
    ]
}